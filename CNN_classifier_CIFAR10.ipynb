{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import classifier_utils\n",
    "from classifier_utils import cnn_model, preprocess_img, get_class\n",
    "%matplotlib inline\n",
    "import glob\n",
    "import os\n",
    "from skimage import io\n",
    "from skimage import transform\n",
    "import keras\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.models import Sequential, Model\n",
    "import vgg16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (y_train[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Array details (dimension, type):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x_train.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[  59.   62.   63.]\n",
      "   [  43.   46.   45.]\n",
      "   [  50.   48.   43.]\n",
      "   ..., \n",
      "   [ 158.  132.  108.]\n",
      "   [ 152.  125.  102.]\n",
      "   [ 148.  124.  103.]]\n",
      "\n",
      "  [[  16.   20.   20.]\n",
      "   [   0.    0.    0.]\n",
      "   [  18.    8.    0.]\n",
      "   ..., \n",
      "   [ 123.   88.   55.]\n",
      "   [ 119.   83.   50.]\n",
      "   [ 122.   87.   57.]]\n",
      "\n",
      "  [[  25.   24.   21.]\n",
      "   [  16.    7.    0.]\n",
      "   [  49.   27.    8.]\n",
      "   ..., \n",
      "   [ 118.   84.   50.]\n",
      "   [ 120.   84.   50.]\n",
      "   [ 109.   73.   42.]]\n",
      "\n",
      "  ..., \n",
      "  [[ 208.  170.   96.]\n",
      "   [ 201.  153.   34.]\n",
      "   [ 198.  161.   26.]\n",
      "   ..., \n",
      "   [ 160.  133.   70.]\n",
      "   [  56.   31.    7.]\n",
      "   [  53.   34.   20.]]\n",
      "\n",
      "  [[ 180.  139.   96.]\n",
      "   [ 173.  123.   42.]\n",
      "   [ 186.  144.   30.]\n",
      "   ..., \n",
      "   [ 184.  148.   94.]\n",
      "   [  97.   62.   34.]\n",
      "   [  83.   53.   34.]]\n",
      "\n",
      "  [[ 177.  144.  116.]\n",
      "   [ 168.  129.   94.]\n",
      "   [ 179.  142.   87.]\n",
      "   ..., \n",
      "   [ 216.  184.  140.]\n",
      "   [ 151.  118.   84.]\n",
      "   [ 123.   92.   72.]]]\n",
      "\n",
      "\n",
      " [[[ 154.  177.  187.]\n",
      "   [ 126.  137.  136.]\n",
      "   [ 105.  104.   95.]\n",
      "   ..., \n",
      "   [  91.   95.   71.]\n",
      "   [  87.   90.   71.]\n",
      "   [  79.   81.   70.]]\n",
      "\n",
      "  [[ 140.  160.  169.]\n",
      "   [ 145.  153.  154.]\n",
      "   [ 125.  125.  118.]\n",
      "   ..., \n",
      "   [  96.   99.   78.]\n",
      "   [  77.   80.   62.]\n",
      "   [  71.   73.   61.]]\n",
      "\n",
      "  [[ 140.  155.  164.]\n",
      "   [ 139.  146.  149.]\n",
      "   [ 115.  115.  112.]\n",
      "   ..., \n",
      "   [  79.   82.   64.]\n",
      "   [  68.   70.   55.]\n",
      "   [  67.   69.   55.]]\n",
      "\n",
      "  ..., \n",
      "  [[ 175.  167.  166.]\n",
      "   [ 156.  154.  160.]\n",
      "   [ 154.  160.  170.]\n",
      "   ..., \n",
      "   [  42.   34.   36.]\n",
      "   [  61.   53.   57.]\n",
      "   [  93.   83.   91.]]\n",
      "\n",
      "  [[ 165.  154.  128.]\n",
      "   [ 156.  152.  130.]\n",
      "   [ 159.  161.  142.]\n",
      "   ..., \n",
      "   [ 103.   93.   96.]\n",
      "   [ 123.  114.  120.]\n",
      "   [ 131.  121.  131.]]\n",
      "\n",
      "  [[ 163.  148.  120.]\n",
      "   [ 158.  148.  122.]\n",
      "   [ 163.  156.  133.]\n",
      "   ..., \n",
      "   [ 143.  133.  139.]\n",
      "   [ 143.  134.  142.]\n",
      "   [ 143.  133.  144.]]]\n",
      "\n",
      "\n",
      " [[[ 255.  255.  255.]\n",
      "   [ 253.  253.  253.]\n",
      "   [ 253.  253.  253.]\n",
      "   ..., \n",
      "   [ 253.  253.  253.]\n",
      "   [ 253.  253.  253.]\n",
      "   [ 253.  253.  253.]]\n",
      "\n",
      "  [[ 255.  255.  255.]\n",
      "   [ 255.  255.  255.]\n",
      "   [ 255.  255.  255.]\n",
      "   ..., \n",
      "   [ 255.  255.  255.]\n",
      "   [ 255.  255.  255.]\n",
      "   [ 255.  255.  255.]]\n",
      "\n",
      "  [[ 255.  255.  255.]\n",
      "   [ 254.  254.  254.]\n",
      "   [ 254.  254.  254.]\n",
      "   ..., \n",
      "   [ 254.  254.  254.]\n",
      "   [ 254.  254.  254.]\n",
      "   [ 254.  254.  254.]]\n",
      "\n",
      "  ..., \n",
      "  [[ 113.  120.  112.]\n",
      "   [ 111.  118.  111.]\n",
      "   [ 105.  112.  106.]\n",
      "   ..., \n",
      "   [  72.   81.   80.]\n",
      "   [  72.   80.   79.]\n",
      "   [  72.   80.   79.]]\n",
      "\n",
      "  [[ 111.  118.  110.]\n",
      "   [ 104.  111.  104.]\n",
      "   [  99.  106.   98.]\n",
      "   ..., \n",
      "   [  68.   75.   73.]\n",
      "   [  70.   76.   75.]\n",
      "   [  78.   84.   82.]]\n",
      "\n",
      "  [[ 106.  113.  105.]\n",
      "   [  99.  106.   98.]\n",
      "   [  95.  102.   94.]\n",
      "   ..., \n",
      "   [  78.   85.   83.]\n",
      "   [  79.   85.   83.]\n",
      "   [  80.   86.   84.]]]\n",
      "\n",
      "\n",
      " ..., \n",
      " [[[  35.  178.  235.]\n",
      "   [  40.  176.  239.]\n",
      "   [  42.  176.  241.]\n",
      "   ..., \n",
      "   [  99.  177.  219.]\n",
      "   [  79.  147.  197.]\n",
      "   [  89.  148.  189.]]\n",
      "\n",
      "  [[  57.  182.  234.]\n",
      "   [  44.  184.  250.]\n",
      "   [  50.  183.  240.]\n",
      "   ..., \n",
      "   [ 156.  182.  200.]\n",
      "   [ 141.  177.  206.]\n",
      "   [ 116.  149.  175.]]\n",
      "\n",
      "  [[  98.  197.  237.]\n",
      "   [  64.  189.  252.]\n",
      "   [  69.  192.  245.]\n",
      "   ..., \n",
      "   [ 188.  195.  206.]\n",
      "   [ 119.  135.  147.]\n",
      "   [  61.   79.   90.]]\n",
      "\n",
      "  ..., \n",
      "  [[  73.   79.   77.]\n",
      "   [  53.   63.   68.]\n",
      "   [  54.   68.   80.]\n",
      "   ..., \n",
      "   [  17.   40.   64.]\n",
      "   [  21.   36.   51.]\n",
      "   [  33.   48.   49.]]\n",
      "\n",
      "  [[  61.   68.   75.]\n",
      "   [  55.   70.   86.]\n",
      "   [  57.   79.  103.]\n",
      "   ..., \n",
      "   [  24.   48.   72.]\n",
      "   [  17.   35.   53.]\n",
      "   [   7.   23.   32.]]\n",
      "\n",
      "  [[  44.   56.   73.]\n",
      "   [  46.   66.   88.]\n",
      "   [  49.   77.  105.]\n",
      "   ..., \n",
      "   [  27.   52.   77.]\n",
      "   [  21.   43.   66.]\n",
      "   [  12.   31.   50.]]]\n",
      "\n",
      "\n",
      " [[[ 189.  211.  240.]\n",
      "   [ 186.  208.  236.]\n",
      "   [ 185.  207.  235.]\n",
      "   ..., \n",
      "   [ 175.  195.  224.]\n",
      "   [ 172.  194.  222.]\n",
      "   [ 169.  194.  220.]]\n",
      "\n",
      "  [[ 194.  210.  239.]\n",
      "   [ 191.  207.  236.]\n",
      "   [ 190.  206.  235.]\n",
      "   ..., \n",
      "   [ 173.  192.  220.]\n",
      "   [ 171.  191.  218.]\n",
      "   [ 167.  190.  216.]]\n",
      "\n",
      "  [[ 208.  219.  244.]\n",
      "   [ 205.  216.  240.]\n",
      "   [ 204.  215.  239.]\n",
      "   ..., \n",
      "   [ 175.  191.  217.]\n",
      "   [ 172.  190.  216.]\n",
      "   [ 169.  191.  215.]]\n",
      "\n",
      "  ..., \n",
      "  [[ 207.  199.  181.]\n",
      "   [ 203.  195.  175.]\n",
      "   [ 203.  196.  173.]\n",
      "   ..., \n",
      "   [ 135.  132.  127.]\n",
      "   [ 162.  158.  150.]\n",
      "   [ 168.  163.  151.]]\n",
      "\n",
      "  [[ 198.  190.  170.]\n",
      "   [ 189.  181.  159.]\n",
      "   [ 180.  172.  147.]\n",
      "   ..., \n",
      "   [ 178.  171.  160.]\n",
      "   [ 175.  169.  156.]\n",
      "   [ 175.  169.  154.]]\n",
      "\n",
      "  [[ 198.  189.  173.]\n",
      "   [ 189.  181.  162.]\n",
      "   [ 178.  170.  149.]\n",
      "   ..., \n",
      "   [ 195.  184.  169.]\n",
      "   [ 196.  189.  171.]\n",
      "   [ 195.  190.  171.]]]\n",
      "\n",
      "\n",
      " [[[ 229.  229.  239.]\n",
      "   [ 236.  237.  247.]\n",
      "   [ 234.  236.  247.]\n",
      "   ..., \n",
      "   [ 217.  219.  233.]\n",
      "   [ 221.  223.  234.]\n",
      "   [ 222.  223.  233.]]\n",
      "\n",
      "  [[ 222.  221.  229.]\n",
      "   [ 239.  239.  249.]\n",
      "   [ 233.  234.  246.]\n",
      "   ..., \n",
      "   [ 223.  223.  236.]\n",
      "   [ 227.  228.  238.]\n",
      "   [ 210.  211.  220.]]\n",
      "\n",
      "  [[ 213.  206.  211.]\n",
      "   [ 234.  232.  239.]\n",
      "   [ 231.  233.  244.]\n",
      "   ..., \n",
      "   [ 220.  220.  232.]\n",
      "   [ 220.  219.  232.]\n",
      "   [ 202.  203.  215.]]\n",
      "\n",
      "  ..., \n",
      "  [[ 150.  143.  135.]\n",
      "   [ 140.  135.  127.]\n",
      "   [ 132.  127.  120.]\n",
      "   ..., \n",
      "   [ 224.  222.  218.]\n",
      "   [ 230.  228.  225.]\n",
      "   [ 241.  241.  238.]]\n",
      "\n",
      "  [[ 137.  132.  126.]\n",
      "   [ 130.  127.  120.]\n",
      "   [ 125.  121.  115.]\n",
      "   ..., \n",
      "   [ 181.  180.  178.]\n",
      "   [ 202.  201.  198.]\n",
      "   [ 212.  211.  207.]]\n",
      "\n",
      "  [[ 122.  119.  114.]\n",
      "   [ 118.  116.  110.]\n",
      "   [ 120.  116.  111.]\n",
      "   ..., \n",
      "   [ 179.  177.  173.]\n",
      "   [ 164.  164.  162.]\n",
      "   [ 163.  163.  161.]]]]\n"
     ]
    }
   ],
   "source": [
    "print (x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Switch RGB to BGR order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = x_train[:, :, :, ::-1]  \n",
    "\n",
    "# Subtract ImageNet mean pixel \n",
    "x_train[:, :, :, 0] -= 103\n",
    "x_train[:, :, :, 1] -= 116\n",
    "x_train[:, :, :, 2] -= 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ -40.  -54.  -64.]\n",
      "   [ -58.  -70.  -80.]\n",
      "   [ -60.  -68.  -73.]\n",
      "   ..., \n",
      "   [   5.   16.   35.]\n",
      "   [  -1.    9.   29.]\n",
      "   [   0.    8.   25.]]\n",
      "\n",
      "  [[ -83.  -96. -107.]\n",
      "   [-103. -116. -123.]\n",
      "   [-103. -108. -105.]\n",
      "   ..., \n",
      "   [ -48.  -28.    0.]\n",
      "   [ -53.  -33.   -4.]\n",
      "   [ -46.  -29.   -1.]]\n",
      "\n",
      "  [[ -82.  -92.  -98.]\n",
      "   [-103. -109. -107.]\n",
      "   [ -95.  -89.  -74.]\n",
      "   ..., \n",
      "   [ -53.  -32.   -5.]\n",
      "   [ -53.  -32.   -3.]\n",
      "   [ -61.  -43.  -14.]]\n",
      "\n",
      "  ..., \n",
      "  [[  -7.   54.   85.]\n",
      "   [ -69.   37.   78.]\n",
      "   [ -77.   45.   75.]\n",
      "   ..., \n",
      "   [ -33.   17.   37.]\n",
      "   [ -96.  -85.  -67.]\n",
      "   [ -83.  -82.  -70.]]\n",
      "\n",
      "  [[  -7.   23.   57.]\n",
      "   [ -61.    7.   50.]\n",
      "   [ -73.   28.   63.]\n",
      "   ..., \n",
      "   [  -9.   32.   61.]\n",
      "   [ -69.  -54.  -26.]\n",
      "   [ -69.  -63.  -40.]]\n",
      "\n",
      "  [[  13.   28.   54.]\n",
      "   [  -9.   13.   45.]\n",
      "   [ -16.   26.   56.]\n",
      "   ..., \n",
      "   [  37.   68.   93.]\n",
      "   [ -19.    2.   28.]\n",
      "   [ -31.  -24.    0.]]]\n",
      "\n",
      "\n",
      " [[[  84.   61.   31.]\n",
      "   [  33.   21.    3.]\n",
      "   [  -8.  -12.  -18.]\n",
      "   ..., \n",
      "   [ -32.  -21.  -32.]\n",
      "   [ -32.  -26.  -36.]\n",
      "   [ -33.  -35.  -44.]]\n",
      "\n",
      "  [[  66.   44.   17.]\n",
      "   [  51.   37.   22.]\n",
      "   [  15.    9.    2.]\n",
      "   ..., \n",
      "   [ -25.  -17.  -27.]\n",
      "   [ -41.  -36.  -46.]\n",
      "   [ -42.  -43.  -52.]]\n",
      "\n",
      "  [[  61.   39.   17.]\n",
      "   [  46.   30.   16.]\n",
      "   [   9.   -1.   -8.]\n",
      "   ..., \n",
      "   [ -39.  -34.  -44.]\n",
      "   [ -48.  -46.  -55.]\n",
      "   [ -48.  -47.  -56.]]\n",
      "\n",
      "  ..., \n",
      "  [[  63.   51.   52.]\n",
      "   [  57.   38.   33.]\n",
      "   [  67.   44.   31.]\n",
      "   ..., \n",
      "   [ -67.  -82.  -81.]\n",
      "   [ -46.  -63.  -62.]\n",
      "   [ -12.  -33.  -30.]]\n",
      "\n",
      "  [[  25.   38.   42.]\n",
      "   [  27.   36.   33.]\n",
      "   [  39.   45.   36.]\n",
      "   ..., \n",
      "   [  -7.  -23.  -20.]\n",
      "   [  17.   -2.    0.]\n",
      "   [  28.    5.    8.]]\n",
      "\n",
      "  [[  17.   32.   40.]\n",
      "   [  19.   32.   35.]\n",
      "   [  30.   40.   40.]\n",
      "   ..., \n",
      "   [  36.   17.   20.]\n",
      "   [  39.   18.   20.]\n",
      "   [  41.   17.   20.]]]\n",
      "\n",
      "\n",
      " [[[ 152.  139.  132.]\n",
      "   [ 150.  137.  130.]\n",
      "   [ 150.  137.  130.]\n",
      "   ..., \n",
      "   [ 150.  137.  130.]\n",
      "   [ 150.  137.  130.]\n",
      "   [ 150.  137.  130.]]\n",
      "\n",
      "  [[ 152.  139.  132.]\n",
      "   [ 152.  139.  132.]\n",
      "   [ 152.  139.  132.]\n",
      "   ..., \n",
      "   [ 152.  139.  132.]\n",
      "   [ 152.  139.  132.]\n",
      "   [ 152.  139.  132.]]\n",
      "\n",
      "  [[ 152.  139.  132.]\n",
      "   [ 151.  138.  131.]\n",
      "   [ 151.  138.  131.]\n",
      "   ..., \n",
      "   [ 151.  138.  131.]\n",
      "   [ 151.  138.  131.]\n",
      "   [ 151.  138.  131.]]\n",
      "\n",
      "  ..., \n",
      "  [[   9.    4.  -10.]\n",
      "   [   8.    2.  -12.]\n",
      "   [   3.   -4.  -18.]\n",
      "   ..., \n",
      "   [ -23.  -35.  -51.]\n",
      "   [ -24.  -36.  -51.]\n",
      "   [ -24.  -36.  -51.]]\n",
      "\n",
      "  [[   7.    2.  -12.]\n",
      "   [   1.   -5.  -19.]\n",
      "   [  -5.  -10.  -24.]\n",
      "   ..., \n",
      "   [ -30.  -41.  -55.]\n",
      "   [ -28.  -40.  -53.]\n",
      "   [ -21.  -32.  -45.]]\n",
      "\n",
      "  [[   2.   -3.  -17.]\n",
      "   [  -5.  -10.  -24.]\n",
      "   [  -9.  -14.  -28.]\n",
      "   ..., \n",
      "   [ -20.  -31.  -45.]\n",
      "   [ -20.  -31.  -44.]\n",
      "   [ -19.  -30.  -43.]]]\n",
      "\n",
      "\n",
      " ..., \n",
      " [[[ 132.   62.  -88.]\n",
      "   [ 136.   60.  -83.]\n",
      "   [ 138.   60.  -81.]\n",
      "   ..., \n",
      "   [ 116.   61.  -24.]\n",
      "   [  94.   31.  -44.]\n",
      "   [  86.   32.  -34.]]\n",
      "\n",
      "  [[ 131.   66.  -66.]\n",
      "   [ 147.   68.  -79.]\n",
      "   [ 137.   67.  -73.]\n",
      "   ..., \n",
      "   [  97.   66.   33.]\n",
      "   [ 103.   61.   18.]\n",
      "   [  72.   33.   -7.]]\n",
      "\n",
      "  [[ 134.   81.  -25.]\n",
      "   [ 149.   73.  -59.]\n",
      "   [ 142.   76.  -54.]\n",
      "   ..., \n",
      "   [ 103.   79.   65.]\n",
      "   [  44.   19.   -4.]\n",
      "   [ -13.  -37.  -62.]]\n",
      "\n",
      "  ..., \n",
      "  [[ -26.  -37.  -50.]\n",
      "   [ -35.  -53.  -70.]\n",
      "   [ -23.  -48.  -69.]\n",
      "   ..., \n",
      "   [ -39.  -76. -106.]\n",
      "   [ -52.  -80. -102.]\n",
      "   [ -54.  -68.  -90.]]\n",
      "\n",
      "  [[ -28.  -48.  -62.]\n",
      "   [ -17.  -46.  -68.]\n",
      "   [   0.  -37.  -66.]\n",
      "   ..., \n",
      "   [ -31.  -68.  -99.]\n",
      "   [ -50.  -81. -106.]\n",
      "   [ -71.  -93. -116.]]\n",
      "\n",
      "  [[ -30.  -60.  -79.]\n",
      "   [ -15.  -50.  -77.]\n",
      "   [   2.  -39.  -74.]\n",
      "   ..., \n",
      "   [ -26.  -64.  -96.]\n",
      "   [ -37.  -73. -102.]\n",
      "   [ -53.  -85. -111.]]]\n",
      "\n",
      "\n",
      " [[[ 137.   95.   66.]\n",
      "   [ 133.   92.   63.]\n",
      "   [ 132.   91.   62.]\n",
      "   ..., \n",
      "   [ 121.   79.   52.]\n",
      "   [ 119.   78.   49.]\n",
      "   [ 117.   78.   46.]]\n",
      "\n",
      "  [[ 136.   94.   71.]\n",
      "   [ 133.   91.   68.]\n",
      "   [ 132.   90.   67.]\n",
      "   ..., \n",
      "   [ 117.   76.   50.]\n",
      "   [ 115.   75.   48.]\n",
      "   [ 113.   74.   44.]]\n",
      "\n",
      "  [[ 141.  103.   85.]\n",
      "   [ 137.  100.   82.]\n",
      "   [ 136.   99.   81.]\n",
      "   ..., \n",
      "   [ 114.   75.   52.]\n",
      "   [ 113.   74.   49.]\n",
      "   [ 112.   75.   46.]]\n",
      "\n",
      "  ..., \n",
      "  [[  78.   83.   84.]\n",
      "   [  72.   79.   80.]\n",
      "   [  70.   80.   80.]\n",
      "   ..., \n",
      "   [  24.   16.   12.]\n",
      "   [  47.   42.   39.]\n",
      "   [  48.   47.   45.]]\n",
      "\n",
      "  [[  67.   74.   75.]\n",
      "   [  56.   65.   66.]\n",
      "   [  44.   56.   57.]\n",
      "   ..., \n",
      "   [  57.   55.   55.]\n",
      "   [  53.   53.   52.]\n",
      "   [  51.   53.   52.]]\n",
      "\n",
      "  [[  70.   73.   75.]\n",
      "   [  59.   65.   66.]\n",
      "   [  46.   54.   55.]\n",
      "   ..., \n",
      "   [  66.   68.   72.]\n",
      "   [  68.   73.   73.]\n",
      "   [  68.   74.   72.]]]\n",
      "\n",
      "\n",
      " [[[ 136.  113.  106.]\n",
      "   [ 144.  121.  113.]\n",
      "   [ 144.  120.  111.]\n",
      "   ..., \n",
      "   [ 130.  103.   94.]\n",
      "   [ 131.  107.   98.]\n",
      "   [ 130.  107.   99.]]\n",
      "\n",
      "  [[ 126.  105.   99.]\n",
      "   [ 146.  123.  116.]\n",
      "   [ 143.  118.  110.]\n",
      "   ..., \n",
      "   [ 133.  107.  100.]\n",
      "   [ 135.  112.  104.]\n",
      "   [ 117.   95.   87.]]\n",
      "\n",
      "  [[ 108.   90.   90.]\n",
      "   [ 136.  116.  111.]\n",
      "   [ 141.  117.  108.]\n",
      "   ..., \n",
      "   [ 129.  104.   97.]\n",
      "   [ 129.  103.   97.]\n",
      "   [ 112.   87.   79.]]\n",
      "\n",
      "  ..., \n",
      "  [[  32.   27.   27.]\n",
      "   [  24.   19.   17.]\n",
      "   [  17.   11.    9.]\n",
      "   ..., \n",
      "   [ 115.  106.  101.]\n",
      "   [ 122.  112.  107.]\n",
      "   [ 135.  125.  118.]]\n",
      "\n",
      "  [[  23.   16.   14.]\n",
      "   [  17.   11.    7.]\n",
      "   [  12.    5.    2.]\n",
      "   ..., \n",
      "   [  75.   64.   58.]\n",
      "   [  95.   85.   79.]\n",
      "   [ 104.   95.   89.]]\n",
      "\n",
      "  [[  11.    3.   -1.]\n",
      "   [   7.    0.   -5.]\n",
      "   [   8.    0.   -3.]\n",
      "   ..., \n",
      "   [  70.   61.   56.]\n",
      "   [  59.   48.   41.]\n",
      "   [  58.   47.   40.]]]]\n"
     ]
    }
   ],
   "source": [
    "print (x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image resizing. Vgg16 takes minimum size of 48x48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    }
   ],
   "source": [
    "x_train_vgg = np.zeros((50000,224,224,3))\n",
    "\n",
    "for i in range(50000):\n",
    "    x_train_vgg[i] = transform.resize(x_train[i], (224, 224), order=0, preserve_range=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ -40.  -54.  -64.]\n",
      "   [ -40.  -54.  -64.]\n",
      "   [ -40.  -54.  -64.]\n",
      "   ..., \n",
      "   [   0.    8.   25.]\n",
      "   [   0.    8.   25.]\n",
      "   [   0.    8.   25.]]\n",
      "\n",
      "  [[ -40.  -54.  -64.]\n",
      "   [ -40.  -54.  -64.]\n",
      "   [ -40.  -54.  -64.]\n",
      "   ..., \n",
      "   [   0.    8.   25.]\n",
      "   [   0.    8.   25.]\n",
      "   [   0.    8.   25.]]\n",
      "\n",
      "  [[ -40.  -54.  -64.]\n",
      "   [ -40.  -54.  -64.]\n",
      "   [ -40.  -54.  -64.]\n",
      "   ..., \n",
      "   [   0.    8.   25.]\n",
      "   [   0.    8.   25.]\n",
      "   [   0.    8.   25.]]\n",
      "\n",
      "  ..., \n",
      "  [[  13.   28.   54.]\n",
      "   [  13.   28.   54.]\n",
      "   [  13.   28.   54.]\n",
      "   ..., \n",
      "   [ -31.  -24.    0.]\n",
      "   [ -31.  -24.    0.]\n",
      "   [ -31.  -24.    0.]]\n",
      "\n",
      "  [[  13.   28.   54.]\n",
      "   [  13.   28.   54.]\n",
      "   [  13.   28.   54.]\n",
      "   ..., \n",
      "   [ -31.  -24.    0.]\n",
      "   [ -31.  -24.    0.]\n",
      "   [ -31.  -24.    0.]]\n",
      "\n",
      "  [[  13.   28.   54.]\n",
      "   [  13.   28.   54.]\n",
      "   [  13.   28.   54.]\n",
      "   ..., \n",
      "   [ -31.  -24.    0.]\n",
      "   [ -31.  -24.    0.]\n",
      "   [ -31.  -24.    0.]]]\n",
      "\n",
      "\n",
      " [[[  84.   61.   31.]\n",
      "   [  84.   61.   31.]\n",
      "   [  84.   61.   31.]\n",
      "   ..., \n",
      "   [ -33.  -35.  -44.]\n",
      "   [ -33.  -35.  -44.]\n",
      "   [ -33.  -35.  -44.]]\n",
      "\n",
      "  [[  84.   61.   31.]\n",
      "   [  84.   61.   31.]\n",
      "   [  84.   61.   31.]\n",
      "   ..., \n",
      "   [ -33.  -35.  -44.]\n",
      "   [ -33.  -35.  -44.]\n",
      "   [ -33.  -35.  -44.]]\n",
      "\n",
      "  [[  84.   61.   31.]\n",
      "   [  84.   61.   31.]\n",
      "   [  84.   61.   31.]\n",
      "   ..., \n",
      "   [ -33.  -35.  -44.]\n",
      "   [ -33.  -35.  -44.]\n",
      "   [ -33.  -35.  -44.]]\n",
      "\n",
      "  ..., \n",
      "  [[  17.   32.   40.]\n",
      "   [  17.   32.   40.]\n",
      "   [  17.   32.   40.]\n",
      "   ..., \n",
      "   [  41.   17.   20.]\n",
      "   [  41.   17.   20.]\n",
      "   [  41.   17.   20.]]\n",
      "\n",
      "  [[  17.   32.   40.]\n",
      "   [  17.   32.   40.]\n",
      "   [  17.   32.   40.]\n",
      "   ..., \n",
      "   [  41.   17.   20.]\n",
      "   [  41.   17.   20.]\n",
      "   [  41.   17.   20.]]\n",
      "\n",
      "  [[  17.   32.   40.]\n",
      "   [  17.   32.   40.]\n",
      "   [  17.   32.   40.]\n",
      "   ..., \n",
      "   [  41.   17.   20.]\n",
      "   [  41.   17.   20.]\n",
      "   [  41.   17.   20.]]]\n",
      "\n",
      "\n",
      " [[[ 152.  139.  132.]\n",
      "   [ 152.  139.  132.]\n",
      "   [ 152.  139.  132.]\n",
      "   ..., \n",
      "   [ 150.  137.  130.]\n",
      "   [ 150.  137.  130.]\n",
      "   [ 150.  137.  130.]]\n",
      "\n",
      "  [[ 152.  139.  132.]\n",
      "   [ 152.  139.  132.]\n",
      "   [ 152.  139.  132.]\n",
      "   ..., \n",
      "   [ 150.  137.  130.]\n",
      "   [ 150.  137.  130.]\n",
      "   [ 150.  137.  130.]]\n",
      "\n",
      "  [[ 152.  139.  132.]\n",
      "   [ 152.  139.  132.]\n",
      "   [ 152.  139.  132.]\n",
      "   ..., \n",
      "   [ 150.  137.  130.]\n",
      "   [ 150.  137.  130.]\n",
      "   [ 150.  137.  130.]]\n",
      "\n",
      "  ..., \n",
      "  [[   2.   -3.  -17.]\n",
      "   [   2.   -3.  -17.]\n",
      "   [   2.   -3.  -17.]\n",
      "   ..., \n",
      "   [ -19.  -30.  -43.]\n",
      "   [ -19.  -30.  -43.]\n",
      "   [ -19.  -30.  -43.]]\n",
      "\n",
      "  [[   2.   -3.  -17.]\n",
      "   [   2.   -3.  -17.]\n",
      "   [   2.   -3.  -17.]\n",
      "   ..., \n",
      "   [ -19.  -30.  -43.]\n",
      "   [ -19.  -30.  -43.]\n",
      "   [ -19.  -30.  -43.]]\n",
      "\n",
      "  [[   2.   -3.  -17.]\n",
      "   [   2.   -3.  -17.]\n",
      "   [   2.   -3.  -17.]\n",
      "   ..., \n",
      "   [ -19.  -30.  -43.]\n",
      "   [ -19.  -30.  -43.]\n",
      "   [ -19.  -30.  -43.]]]\n",
      "\n",
      "\n",
      " ..., \n",
      " [[[ 132.   62.  -88.]\n",
      "   [ 132.   62.  -88.]\n",
      "   [ 132.   62.  -88.]\n",
      "   ..., \n",
      "   [  86.   32.  -34.]\n",
      "   [  86.   32.  -34.]\n",
      "   [  86.   32.  -34.]]\n",
      "\n",
      "  [[ 132.   62.  -88.]\n",
      "   [ 132.   62.  -88.]\n",
      "   [ 132.   62.  -88.]\n",
      "   ..., \n",
      "   [  86.   32.  -34.]\n",
      "   [  86.   32.  -34.]\n",
      "   [  86.   32.  -34.]]\n",
      "\n",
      "  [[ 132.   62.  -88.]\n",
      "   [ 132.   62.  -88.]\n",
      "   [ 132.   62.  -88.]\n",
      "   ..., \n",
      "   [  86.   32.  -34.]\n",
      "   [  86.   32.  -34.]\n",
      "   [  86.   32.  -34.]]\n",
      "\n",
      "  ..., \n",
      "  [[ -30.  -60.  -79.]\n",
      "   [ -30.  -60.  -79.]\n",
      "   [ -30.  -60.  -79.]\n",
      "   ..., \n",
      "   [ -53.  -85. -111.]\n",
      "   [ -53.  -85. -111.]\n",
      "   [ -53.  -85. -111.]]\n",
      "\n",
      "  [[ -30.  -60.  -79.]\n",
      "   [ -30.  -60.  -79.]\n",
      "   [ -30.  -60.  -79.]\n",
      "   ..., \n",
      "   [ -53.  -85. -111.]\n",
      "   [ -53.  -85. -111.]\n",
      "   [ -53.  -85. -111.]]\n",
      "\n",
      "  [[ -30.  -60.  -79.]\n",
      "   [ -30.  -60.  -79.]\n",
      "   [ -30.  -60.  -79.]\n",
      "   ..., \n",
      "   [ -53.  -85. -111.]\n",
      "   [ -53.  -85. -111.]\n",
      "   [ -53.  -85. -111.]]]\n",
      "\n",
      "\n",
      " [[[ 137.   95.   66.]\n",
      "   [ 137.   95.   66.]\n",
      "   [ 137.   95.   66.]\n",
      "   ..., \n",
      "   [ 117.   78.   46.]\n",
      "   [ 117.   78.   46.]\n",
      "   [ 117.   78.   46.]]\n",
      "\n",
      "  [[ 137.   95.   66.]\n",
      "   [ 137.   95.   66.]\n",
      "   [ 137.   95.   66.]\n",
      "   ..., \n",
      "   [ 117.   78.   46.]\n",
      "   [ 117.   78.   46.]\n",
      "   [ 117.   78.   46.]]\n",
      "\n",
      "  [[ 137.   95.   66.]\n",
      "   [ 137.   95.   66.]\n",
      "   [ 137.   95.   66.]\n",
      "   ..., \n",
      "   [ 117.   78.   46.]\n",
      "   [ 117.   78.   46.]\n",
      "   [ 117.   78.   46.]]\n",
      "\n",
      "  ..., \n",
      "  [[  70.   73.   75.]\n",
      "   [  70.   73.   75.]\n",
      "   [  70.   73.   75.]\n",
      "   ..., \n",
      "   [  68.   74.   72.]\n",
      "   [  68.   74.   72.]\n",
      "   [  68.   74.   72.]]\n",
      "\n",
      "  [[  70.   73.   75.]\n",
      "   [  70.   73.   75.]\n",
      "   [  70.   73.   75.]\n",
      "   ..., \n",
      "   [  68.   74.   72.]\n",
      "   [  68.   74.   72.]\n",
      "   [  68.   74.   72.]]\n",
      "\n",
      "  [[  70.   73.   75.]\n",
      "   [  70.   73.   75.]\n",
      "   [  70.   73.   75.]\n",
      "   ..., \n",
      "   [  68.   74.   72.]\n",
      "   [  68.   74.   72.]\n",
      "   [  68.   74.   72.]]]\n",
      "\n",
      "\n",
      " [[[ 136.  113.  106.]\n",
      "   [ 136.  113.  106.]\n",
      "   [ 136.  113.  106.]\n",
      "   ..., \n",
      "   [ 130.  107.   99.]\n",
      "   [ 130.  107.   99.]\n",
      "   [ 130.  107.   99.]]\n",
      "\n",
      "  [[ 136.  113.  106.]\n",
      "   [ 136.  113.  106.]\n",
      "   [ 136.  113.  106.]\n",
      "   ..., \n",
      "   [ 130.  107.   99.]\n",
      "   [ 130.  107.   99.]\n",
      "   [ 130.  107.   99.]]\n",
      "\n",
      "  [[ 136.  113.  106.]\n",
      "   [ 136.  113.  106.]\n",
      "   [ 136.  113.  106.]\n",
      "   ..., \n",
      "   [ 130.  107.   99.]\n",
      "   [ 130.  107.   99.]\n",
      "   [ 130.  107.   99.]]\n",
      "\n",
      "  ..., \n",
      "  [[  11.    3.   -1.]\n",
      "   [  11.    3.   -1.]\n",
      "   [  11.    3.   -1.]\n",
      "   ..., \n",
      "   [  58.   47.   40.]\n",
      "   [  58.   47.   40.]\n",
      "   [  58.   47.   40.]]\n",
      "\n",
      "  [[  11.    3.   -1.]\n",
      "   [  11.    3.   -1.]\n",
      "   [  11.    3.   -1.]\n",
      "   ..., \n",
      "   [  58.   47.   40.]\n",
      "   [  58.   47.   40.]\n",
      "   [  58.   47.   40.]]\n",
      "\n",
      "  [[  11.    3.   -1.]\n",
      "   [  11.    3.   -1.]\n",
      "   [  11.    3.   -1.]\n",
      "   ..., \n",
      "   [  58.   47.   40.]\n",
      "   [  58.   47.   40.]\n",
      "   [  58.   47.   40.]]]]\n"
     ]
    }
   ],
   "source": [
    "print (x_train_vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1st model\n",
    "#IMG_SIZE = 32\n",
    "#2nd model\n",
    "IMG_SIZE = 224 \n",
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing amount of pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2nd model\n",
    "x_train_vgg = x_train_vgg[0:50000] #bigger value\n",
    "y_train_vgg = y_train[0:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "print (x_train_vgg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why already calculated values? By default on .resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (x_train_vgg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples from training set (224x224 and 32x32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8fcd128208>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAAD8CAYAAAB+fLH0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncgrOGH/QDQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8fcd1b16d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "single_image = x_train_vgg[28:29]\n",
    "plt.imshow(single_image[0])         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_image = x_train[58:59]\n",
    "plt.imshow(single_image[0])         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zdjecie warstwy: include_top=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#first_model = cnn_model(IMG_SIZE,NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_model = keras.applications.vgg16.VGG16(include_top=False, weights='imagenet', input_tensor=None, input_shape=(224,224,3), pooling=max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose model (remember to set NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model = first_model\n",
    "model = second_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.topology.InputLayer at 0x7f8fcd13e128>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f8fcd1645f8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f8fcd164a20>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7f8fcd155d68>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f8fcd0b4710>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f8fcd0b4438>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7f8fcd0adcc0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f8fcd06c2e8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f8fcd06c6d8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f8fcd0069e8>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7f8fcd0141d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f8fcd0332b0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f8fcd03fcc0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f8fccfcd2e8>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7f8fccfe8f28>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f8fccf86588>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f8fccf86e48>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f8fccfa3c88>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7f8fccfbf978>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.layers[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting all layers to trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in model.layers: layer.trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative way to add last layer and activation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model.add(Dense(NUM_CLASSES, activation='softmax')) .add doesn't work for VGG16\n",
    "\n",
    "last = model.output\n",
    "\n",
    "x = Flatten()(last)\n",
    "preds = Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "\n",
    "model = Model(model.input, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                250890    \n",
      "=================================================================\n",
      "Total params: 14,965,578\n",
      "Trainable params: 14,965,578\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set optimizer for tweaks (leraning rate lr=0.001 to lr=0.0001) !Unquote Adam!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Adam=keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam,loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[128,64,112,112]\n\t [[Node: gradients/block1_pool/MaxPool_grad/MaxPoolGrad = MaxPoolGrad[T=DT_FLOAT, _class=[\"loc:@block1_pool/MaxPool\"], data_format=\"NHWC\", ksize=[1, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](block1_conv2/Relu, block1_pool/MaxPool, gradients/block2_conv1/convolution_grad/Conv2DBackpropInput)]]\n\nCaused by op 'gradients/block1_pool/MaxPool_grad/MaxPoolGrad', defined at:\n  File \"/usr/local/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/local/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.5/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2808, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-24-2f3b5eb9fd40>\", line 5, in <module>\n    model.fit(x_train_vgg, y_train, batch_size=128, epochs=1)\n  File \"/usr/local/lib/python3.5/site-packages/keras/engine/training.py\", line 1413, in fit\n    self._make_train_function()\n  File \"/usr/local/lib/python3.5/site-packages/keras/engine/training.py\", line 937, in _make_train_function\n    self.total_loss)\n  File \"/usr/local/lib/python3.5/site-packages/keras/optimizers.py\", line 404, in get_updates\n    grads = self.get_gradients(loss, params)\n  File \"/usr/local/lib/python3.5/site-packages/keras/optimizers.py\", line 71, in get_gradients\n    grads = K.gradients(loss, params)\n  File \"/usr/local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 2305, in gradients\n    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\", line 560, in gradients\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\", line 368, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\", line 560, in <lambda>\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/ops/nn_grad.py\", line 438, in _MaxPoolGrad\n    data_format=op.get_attr(\"data_format\"))\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 1737, in _max_pool_grad\n    data_format=data_format, name=name)\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\n...which was originally created as op 'block1_pool/MaxPool', defined at:\n  File \"/usr/local/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 16 identical lines from previous traceback]\n  File \"/usr/local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-16-45489d500130>\", line 1, in <module>\n    second_model = keras.applications.vgg16.VGG16(include_top=False, weights='imagenet', input_tensor=None, input_shape=(224,224,3), pooling=max)\n  File \"/usr/local/lib/python3.5/site-packages/keras/applications/vgg16.py\", line 113, in VGG16\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n  File \"/usr/local/lib/python3.5/site-packages/keras/engine/topology.py\", line 596, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/usr/local/lib/python3.5/site-packages/keras/layers/pooling.py\", line 154, in call\n    data_format=self.data_format)\n  File \"/usr/local/lib/python3.5/site-packages/keras/layers/pooling.py\", line 217, in _pooling_function\n    pool_mode='max')\n  File \"/usr/local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 3378, in pool2d\n    x = tf.nn.max_pool(x, pool_size, strides, padding=padding)\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\", line 1821, in max_pool\n    name=name)\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 1638, in _max_pool\n    data_format=data_format, name=name)\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[128,64,112,112]\n\t [[Node: gradients/block1_pool/MaxPool_grad/MaxPoolGrad = MaxPoolGrad[T=DT_FLOAT, _class=[\"loc:@block1_pool/MaxPool\"], data_format=\"NHWC\", ksize=[1, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](block1_conv2/Relu, block1_pool/MaxPool, gradients/block2_conv1/convolution_grad/Conv2DBackpropInput)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[128,64,112,112]\n\t [[Node: gradients/block1_pool/MaxPool_grad/MaxPoolGrad = MaxPoolGrad[T=DT_FLOAT, _class=[\"loc:@block1_pool/MaxPool\"], data_format=\"NHWC\", ksize=[1, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](block1_conv2/Relu, block1_pool/MaxPool, gradients/block2_conv1/convolution_grad/Conv2DBackpropInput)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-cf157638f293>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#2nd model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#model.fit(x_train_vgg, y_train_vgg, batch_size=128, epochs=2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_vgg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_vgg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1428\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1430\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1077\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1080\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2266\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2267\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2268\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[128,64,112,112]\n\t [[Node: gradients/block1_pool/MaxPool_grad/MaxPoolGrad = MaxPoolGrad[T=DT_FLOAT, _class=[\"loc:@block1_pool/MaxPool\"], data_format=\"NHWC\", ksize=[1, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](block1_conv2/Relu, block1_pool/MaxPool, gradients/block2_conv1/convolution_grad/Conv2DBackpropInput)]]\n\nCaused by op 'gradients/block1_pool/MaxPool_grad/MaxPoolGrad', defined at:\n  File \"/usr/local/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/local/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.5/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2808, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-24-2f3b5eb9fd40>\", line 5, in <module>\n    model.fit(x_train_vgg, y_train, batch_size=128, epochs=1)\n  File \"/usr/local/lib/python3.5/site-packages/keras/engine/training.py\", line 1413, in fit\n    self._make_train_function()\n  File \"/usr/local/lib/python3.5/site-packages/keras/engine/training.py\", line 937, in _make_train_function\n    self.total_loss)\n  File \"/usr/local/lib/python3.5/site-packages/keras/optimizers.py\", line 404, in get_updates\n    grads = self.get_gradients(loss, params)\n  File \"/usr/local/lib/python3.5/site-packages/keras/optimizers.py\", line 71, in get_gradients\n    grads = K.gradients(loss, params)\n  File \"/usr/local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 2305, in gradients\n    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\", line 560, in gradients\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\", line 368, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\", line 560, in <lambda>\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/ops/nn_grad.py\", line 438, in _MaxPoolGrad\n    data_format=op.get_attr(\"data_format\"))\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 1737, in _max_pool_grad\n    data_format=data_format, name=name)\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\n...which was originally created as op 'block1_pool/MaxPool', defined at:\n  File \"/usr/local/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 16 identical lines from previous traceback]\n  File \"/usr/local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-16-45489d500130>\", line 1, in <module>\n    second_model = keras.applications.vgg16.VGG16(include_top=False, weights='imagenet', input_tensor=None, input_shape=(224,224,3), pooling=max)\n  File \"/usr/local/lib/python3.5/site-packages/keras/applications/vgg16.py\", line 113, in VGG16\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n  File \"/usr/local/lib/python3.5/site-packages/keras/engine/topology.py\", line 596, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/usr/local/lib/python3.5/site-packages/keras/layers/pooling.py\", line 154, in call\n    data_format=self.data_format)\n  File \"/usr/local/lib/python3.5/site-packages/keras/layers/pooling.py\", line 217, in _pooling_function\n    pool_mode='max')\n  File \"/usr/local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 3378, in pool2d\n    x = tf.nn.max_pool(x, pool_size, strides, padding=padding)\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\", line 1821, in max_pool\n    name=name)\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 1638, in _max_pool\n    data_format=data_format, name=name)\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[128,64,112,112]\n\t [[Node: gradients/block1_pool/MaxPool_grad/MaxPoolGrad = MaxPoolGrad[T=DT_FLOAT, _class=[\"loc:@block1_pool/MaxPool\"], data_format=\"NHWC\", ksize=[1, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](block1_conv2/Relu, block1_pool/MaxPool, gradients/block2_conv1/convolution_grad/Conv2DBackpropInput)]]\n"
     ]
    }
   ],
   "source": [
    "#1st model\n",
    "#model.fit(x_train, y_train, batch_size=128, epochs=10)\n",
    "#2nd model\n",
    "#model.fit(x_train_vgg, y_train_vgg, batch_size=128, epochs=2)\n",
    "model.fit(x_train_vgg, y_train_vgg, batch_size=128, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set test size to 64x64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test_vgg = np.zeros((10000,224,224,3))\n",
    "\n",
    "for i in range(10000):\n",
    "    x_test_vgg[i] = transform.resize(x_test[i], (224, 224), order=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (x_test_vgg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.evaluate(x_test_vgg, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
